{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QdM3Kc7ODm34POsyXxwFKh9uCg4EKQrg",
      "authorship_tag": "ABX9TyPPOk4Ic7gN+HqbrGaZ+r7P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MAKKAJIYASHASREE/NLP-PROJECT/blob/main/MCQ_GENERATION_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRSF-QZxlEHr",
        "outputId": "70614e46-6b1c-4011-da55-0c3b8f9ba0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bert-extractive-summarizer\n",
            "  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (1.0.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (3.4.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.7.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.64.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.10.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.6.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.0.10)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.11.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (8.1.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (21.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy->bert-extractive-summarizer) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy->bert-extractive-summarizer) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy->bert-extractive-summarizer) (5.2.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->bert-extractive-summarizer) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy->bert-extractive-summarizer) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy->bert-extractive-summarizer) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (2.0.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (6.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 54.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (4.13.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.8.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 38.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers, huggingface-hub, transformers, bert-extractive-summarizer\n",
            "Successfully installed bert-extractive-summarizer-0.10.1 huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install bert-extractive-summarizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizer import Summarizer,TransformerSummarizer"
      ],
      "metadata": {
        "id": "pISjRPzIlJi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/egypt.txt','r') as f:\n",
        "     text = f.read()"
      ],
      "metadata": {
        "id": "pFBrhybYlbQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Summarizer()\n",
        "result = model(text, min_length=60,max_length=500,ratio=0.4)\n",
        "summary = \"\".join(result)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_dWcHrVl5gk",
        "outputId": "6a56ea66-aa22-426f-a6af-2a56e7c78562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Nile River fed Egyptian civilization for hundreds of years. It begins near the equator in Africa and flows north to the Mediterranean Sea. This soil was fertile, which means it was good for growing crops. The red land was the barren desert beyond the fertile region. When the birds arrived, the annual flood waters would soon follow. Then they used a tool called a shaduf to spread the water across the fields. They were the first to grind wheat into flour and to mix the flour with yeast and water to make dough rise into bread. Egyptians often painted walls white to reflect the blazing heat. Poorer Egyptians simply went to the roof to cool off after sunset. Nubia was the Egyptian name for the area of the upper Nile that had the richest gold mines in Africa. Even during the cool season, chipping minerals out of the rock was miserable work. One ancient painting even shows a man ready to hit a catfish with a wooden hammer. A boomerang is a curved stick that returns to the person who threw it.) Eventually, Egyptians equipped their reed boats with sails and oars. Going south, they raised a sail and let the winds that blew in that direction push them. The Nile provided so well for Egyptians that sometimes they had surpluses, or more goods than they needed. Ancient Egypt had no money, so people exchanged goods that they grew or made. This prosperity made life easier and provided greater opportunities for many Egyptians. For example, some ancient Egyptians learned to be scribes, people whose job was to write and keep records. Some skilled artisans erected stone or brick houses and temples. These traders took Egyptian products such as scrolls, linen, gold, and jewelry. Egyptians created a government that divided the empire into 42 provinces. Priests followed formal rituals and took care of the temples. Before entering a temple, a priest bathed and put on special linen garments and white sandals. So the ruler and the priests tried hard to keep the gods happy. By doing so, they hoped to maintain the social and political order. In Egypt, people became slaves if they owed a debt, committed a crime, or were captured in war. Unlike other ancient African cultures, in Egyptian society men and women had fairly equal rights. For example, they could both own and manage their own property. Children in Egypt played with toys such as dolls, animal figures, board games, and marbles. Almost all Egyptians married when they were in their early teens. Doctors believed that the heart controlled thought and the brain circulated blood, which is the opposite of what is known now. Early Egyptians created a hieroglyphic system with about 700 characters. Egyptians cut the stems into strips, pressed them, and dried them into sheets that could be rolled into scrolls. Legend says a king named Narmer united Upper and Lower Egypt. Some historians think Narmer actually represents several kings who gradually joined the two lands. It combined the red Crown of Lower Egypt with the white Crown of Upper Egypt. When a king died, one of his children usually took his place as ruler. The order in which members of a royal family inherit a throne is called the succession. Historians divide ancient Egyptian dynasties into the Old Kingdom, the Middle Kingdom, and the New Kingdom. Because the pharaoh was thought to be a god, government and religion were not separate in ancient Egypt. The first rulers of Egypt were often buried in an underground tomb topped by mud brick. They replaced the mud brick with a small pyramid of brick or stone. It is called a step pyramid because its sides rise in a series of giant steps. He ordered the construction of the largest pyramid ever built. Miners cut the huge blocks of stone using copper saws and chisels. One reason is that the pyramids drew attention to the tombs inside them. Grave robbers broke into the tombs to steal the treasure buried with the pharaohs. Egyptians believed that if a tomb was robbed, the person buried there could not have a happy afterlife. During the New Kingdom, pharaohs began building more secret tombs in an area called the Valley of the Kings. This was to confuse grave robbers about which passage to take. Sometimes relatives, such as the queen, were buried in the extra rooms. Tombs were supposed to be the palaces of pharaohs in the afterlife. Mourners filled the tomb with objects ranging from food to furniture that the mummified pharaoh would need. A sculpture of a dead pharaoh had “perfect” features, no matter how he really looked. Such activities included growing and preparing food, caring for animals, and building boats. Only a secret tomb built for a New Kingdom pharaoh was ever found with much of its treasure untouched. The dazzling riches found in this tomb show how much wealth the pharaohs spent preparing for the afterlife. By about 2130 B.C., Egyptian kings began to lose their power to local rulers of the provinces.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/boudinfl/pke.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhmJ-giAmDFB",
        "outputId": "0657fce2-6d7f-444f-bf70-de566eeb7711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/boudinfl/pke.git\n",
            "  Cloning https://github.com/boudinfl/pke.git to /tmp/pip-req-build-n1iykpbt\n",
            "  Running command git clone -q https://github.com/boudinfl/pke.git /tmp/pip-req-build-n1iykpbt\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (3.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (2.6.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.7.3)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (1.2.0)\n",
            "Requirement already satisfied: spacy>=3.2.3 in /usr/local/lib/python3.7/dist-packages (from pke==2.0.0) (3.4.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.10.1)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.4.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.4.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.23.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.10.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (3.0.8)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (0.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (21.3)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.1.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (8.1.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (1.0.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (4.64.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.3->pke==2.0.0) (2.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.2.3->pke==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy>=3.2.3->pke==2.0.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.2.3->pke==2.0.0) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.3->pke==2.0.0) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.3->pke==2.0.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.3->pke==2.0.0) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy>=3.2.3->pke==2.0.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.2.3->pke==2.0.0) (2.0.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pke==2.0.0) (2022.6.2)\n",
            "Building wheels for collected packages: pke, sklearn\n",
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pke: filename=pke-2.0.0-py3-none-any.whl size=6160277 sha256=ed80afc58044ed527bd39d07c7f3f6d86fbb1ef211b236fbf9e102b18abe0425\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8n7zupzf/wheels/fa/b3/09/612ee93bf3ee4164bcd5783e742942cdfc892a86039d3e0a33\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=067b3414ac1580c8de32d4c166d609b8f959a8d96326b84a8a45d263018c9df8\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/56/cc/4a8bf86613aafd5b7f1b310477667c1fca5c51c3ae4124a003\n",
            "Successfully built pke sklearn\n",
            "Installing collected packages: unidecode, sklearn, pke\n",
            "Successfully installed pke-2.0.0 sklearn-0.0.post1 unidecode-1.3.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MpPowzhmfKm",
        "outputId": "40cd56f8-cd4b-467a-8bf4-bca49ad6c2c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import pke\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def extracting_keywords(text):\n",
        "    keywords = []\n",
        "    #initialize extractor\n",
        "    extractor = pke.unsupervised.MultipartiteRank()\n",
        "    stoplist = list(string.punctuation)\n",
        "    stoplist+= stopwords.words('english')\n",
        "    stoplist+= ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "    extractor.load_document(text,stoplist=stoplist)\n",
        "    #we want to extract proper noun\n",
        "    pos = {'PROPN'}\n",
        "    extractor.candidate_selection(pos=pos)\n",
        "    #define stopwords and others\n",
        "   \n",
        "    extractor.candidate_weighting()\n",
        "    keyphrases = extractor.get_n_best(n=15)\n",
        "    for i in keyphrases:\n",
        "        keywords.append(i[0])\n",
        "    return keywords\n",
        "\n",
        "key=extracting_keywords(text)\n",
        "print(key)\n",
        "filtered_keys=[]\n",
        "for keyword in key:\n",
        "    if keyword.lower() in summary.lower():\n",
        "        filtered_keys.append(keyword)\n",
        "        \n",
        "print (filtered_keys)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMbThNw2lQ83",
        "outputId": "45b82e85-3c1f-4571-8f2f-e4a03d34a1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/language.py:1899: UserWarning: [W123] Argument disable with value ['ner', 'textcat', 'parser'] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
            "  config_value=config[\"nlp\"][key],\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['egyptians', 'nile river', 'egypt', 'nile', 'euphrates', 'tigris', 'old kingdom', 'red land', 'double crown', 'narmer', 'upper', 'longest river', 'crown', 'africa', 'mediterranean sea']\n",
            "['egyptians', 'nile river', 'egypt', 'nile', 'old kingdom', 'red land', 'narmer', 'upper', 'crown', 'africa', 'mediterranean sea']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flashtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCzXVdhqyjnA",
        "outputId": "b55e42d0-7889-4885-ea6b-7439044f35ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9309 sha256=141330a632b68b493000cb42cfeab094853069a001b55fc6682d04d599f3c30c\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext\n",
            "Successfully installed flashtext-2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hObqts7omvH0",
        "outputId": "d6e9efe5-d562-48a0-ba69-36d984e9970c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from flashtext import KeywordProcessor\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "    sentences = [sent_tokenize(text)]\n",
        "    sentences = [y for x in sentences for y in x]\n",
        "    # Remove any short sentences less than 20 letters.\n",
        "    sentences = [sentence.strip() for sentence in sentences if len(sentence) > 20]\n",
        "    return sentences\n",
        "\n",
        "def get_sentences_for_keyword(key, sentences):\n",
        "    keyword_processor = KeywordProcessor()\n",
        "    keyword_sentences = {}\n",
        "    for word in key:\n",
        "        keyword_sentences[word] = []\n",
        "        keyword_processor.add_keyword(word)\n",
        "    for sentence in sentences:\n",
        "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
        "        for key in keywords_found:\n",
        "            keyword_sentences[key].append(sentence)\n",
        "\n",
        "    for key in keyword_sentences.keys():\n",
        "        values = keyword_sentences[key]\n",
        "        values = sorted(values, key=len, reverse=True)\n",
        "        keyword_sentences[key] = values\n",
        "    return keyword_sentences\n",
        "\n",
        "sentences = tokenize_sentences(summary)\n",
        "keyword_sentence_mapping = get_sentences_for_keyword(filtered_keys, sentences)\n",
        "        \n",
        "print (keyword_sentence_mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1hGjPkeFFmd",
        "outputId": "7d55c16a-6c3d-4f87-e6fb-4881d0b60279"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'egyptians': ['Egyptians cut the stems into strips, pressed them, and dried them into sheets that could be rolled into scrolls.', 'The Nile provided so well for Egyptians that sometimes they had surpluses, or more goods than they needed.', 'For example, some ancient Egyptians learned to be scribes, people whose job was to write and keep records.', 'Egyptians believed that if a tomb was robbed, the person buried there could not have a happy afterlife.', 'This prosperity made life easier and provided greater opportunities for many Egyptians.', 'Egyptians created a government that divided the empire into 42 provinces.', 'Early Egyptians created a hieroglyphic system with about 700 characters.', 'Eventually, Egyptians equipped their reed boats with sails and oars.', 'Poorer Egyptians simply went to the roof to cool off after sunset.', 'Almost all Egyptians married when they were in their early teens.', 'Egyptians often painted walls white to reflect the blazing heat.'], 'nile river': ['The Nile River fed Egyptian civilization for hundreds of years.'], 'egypt': ['Because the pharaoh was thought to be a god, government and religion were not separate in ancient Egypt.', 'In Egypt, people became slaves if they owed a debt, committed a crime, or were captured in war.', 'Children in Egypt played with toys such as dolls, animal figures, board games, and marbles.', 'The first rulers of Egypt were often buried in an underground tomb topped by mud brick.', 'Ancient Egypt had no money, so people exchanged goods that they grew or made.', 'It combined the red Crown of Lower Egypt with the white Crown of Upper Egypt.', 'It combined the red Crown of Lower Egypt with the white Crown of Upper Egypt.', 'Legend says a king named Narmer united Upper and Lower Egypt.'], 'nile': ['The Nile provided so well for Egyptians that sometimes they had surpluses, or more goods than they needed.', 'Nubia was the Egyptian name for the area of the upper Nile that had the richest gold mines in Africa.'], 'old kingdom': ['Historians divide ancient Egyptian dynasties into the Old Kingdom, the Middle Kingdom, and the New Kingdom.'], 'red land': ['The red land was the barren desert beyond the fertile region.'], 'narmer': ['Some historians think Narmer actually represents several kings who gradually joined the two lands.', 'Legend says a king named Narmer united Upper and Lower Egypt.'], 'upper': ['Nubia was the Egyptian name for the area of the upper Nile that had the richest gold mines in Africa.', 'It combined the red Crown of Lower Egypt with the white Crown of Upper Egypt.', 'Legend says a king named Narmer united Upper and Lower Egypt.'], 'crown': ['It combined the red Crown of Lower Egypt with the white Crown of Upper Egypt.', 'It combined the red Crown of Lower Egypt with the white Crown of Upper Egypt.'], 'africa': ['Nubia was the Egyptian name for the area of the upper Nile that had the richest gold mines in Africa.', 'It begins near the equator in Africa and flows north to the Mediterranean Sea.'], 'mediterranean sea': ['It begins near the equator in Africa and flows north to the Mediterranean Sea.']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pywsd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_PO_N8ry1RN",
        "outputId": "8ae22905-70fc-4ffc-b5c6-6ec417474e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pywsd\n",
            "  Downloading pywsd-1.2.5-py3-none-any.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pywsd) (3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.3.5)\n",
            "Collecting wn==0.0.23\n",
            "  Downloading wn-0.0.23.tar.gz (31.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.6 MB 72 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pywsd) (1.21.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->pywsd) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pywsd) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->pywsd) (2022.6)\n",
            "Building wheels for collected packages: wn\n",
            "  Building wheel for wn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wn: filename=wn-0.0.23-py3-none-any.whl size=31792926 sha256=58cbe0e66f1ae71babaf9d49569775d4fb4b40e8a3410cdadcbef9d36b8f378e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/47/17/409766c99dd470f34c512000b90b83f34747c2c975769654d7\n",
            "Successfully built wn\n",
            "Installing collected packages: wn, pywsd\n",
            "Successfully installed pywsd-1.2.5 wn-0.0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDSl6HwMGwgN",
        "outputId": "d82682a6-e97f-44ad-b473-5c2f4b8be8f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "import random\n",
        "from pywsd.similarity import max_similarity\n",
        "from pywsd.lesk import adapted_lesk\n",
        "from pywsd.lesk import simple_lesk\n",
        "from pywsd.lesk import cosine_lesk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Distractors from Wordnet\n",
        "def get_distractors_wordnet(syn,word):\n",
        "    distractors=[]\n",
        "    word= word.lower()\n",
        "    orig_word = word\n",
        "    if len(word.split())>0:\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    hypernym = syn.hypernyms()\n",
        "    if len(hypernym) == 0: \n",
        "        return distractors\n",
        "    for item in hypernym[0].hyponyms():\n",
        "        name = item.lemmas()[0].name()\n",
        "        #print (\"name \",name, \" word\",orig_word)\n",
        "        if name == orig_word:\n",
        "            continue\n",
        "        name = name.replace(\"_\",\" \")\n",
        "        name = \" \".join(w.capitalize() for w in name.split())\n",
        "        if name is not None and name not in distractors:\n",
        "            distractors.append(name)\n",
        "    return distractors\n",
        "\n",
        "def get_wordsense(sent,word):\n",
        "    word= word.lower()\n",
        "    \n",
        "    if len(word.split())>0:\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    \n",
        "    \n",
        "    synsets = wn.synsets(word,'n')\n",
        "    if synsets:\n",
        "        wup = max_similarity(sent, word, 'wup', pos='n')\n",
        "        adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
        "        lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))\n",
        "        return synsets[lowest_index]\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Distractors from http://conceptnet.io/\n",
        "def get_distractors_conceptnet(word):\n",
        "    word = word.lower()\n",
        "    original_word= word\n",
        "    if (len(word.split())>0):\n",
        "        word = word.replace(\" \",\"_\")\n",
        "    distractor_list = [] \n",
        "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
        "    obj = requests.get(url).json()\n",
        "\n",
        "    for edge in obj['edges']:\n",
        "        link = edge['end']['term'] \n",
        "\n",
        "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
        "        obj2 = requests.get(url2).json()\n",
        "        for edge in obj2['edges']:\n",
        "            word2 = edge['start']['label']\n",
        "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
        "                distractor_list.append(word2)\n",
        "                   \n",
        "    return distractor_list\n",
        "\n",
        "key_distractor_list = {}\n",
        "\n",
        "for keyword in keyword_sentence_mapping:\n",
        "    wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)\n",
        "    if wordsense:\n",
        "        distractors = get_distractors_wordnet(wordsense,keyword)\n",
        "        if len(distractors) ==0:\n",
        "            distractors = get_distractors_conceptnet(keyword)\n",
        "        if len(distractors) != 0:\n",
        "            key_distractor_list[keyword] = distractors\n",
        "    else:\n",
        "        \n",
        "        distractors = get_distractors_conceptnet(keyword)\n",
        "        if len(distractors) != 0:\n",
        "            key_distractor_list[keyword] = distractors\n",
        "\n",
        "index = 1\n",
        "for each in key_distractor_list:\n",
        "    sentence = keyword_sentence_mapping[each][0]\n",
        "    pattern = re.compile(each, re.IGNORECASE)\n",
        "    output = pattern.sub( \" _______ \", sentence)\n",
        "    print (\"%s)\"%(index),output)\n",
        "    choices = [each.capitalize()] + key_distractor_list[each]\n",
        "    top4choices = choices[:4]\n",
        "    random.shuffle(top4choices)\n",
        "    optionchoices = ['a','b','c','d']\n",
        "    for idx,choice in enumerate(top4choices):\n",
        "        print (\"\\t\",optionchoices[idx],\")\",\" \",choice)\n",
        "    print (\"\\nMore options: \", choices[4:20],\"\\n\\n\")\n",
        "    index = index + 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBJ3VGCqFvmk",
        "outputId": "e107a733-b5ac-4a57-eed1-be1a8f7eeb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warming up PyWSD (takes ~10 secs)... took 5.777475833892822 secs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1)  _______  cut the stems into strips, pressed them, and dried them into sheets that could be rolled into scrolls.\n",
            "\t a )   Angolan\n",
            "\t b )   Algerian\n",
            "\t c )   Bantu\n",
            "\t d )   Egyptians\n",
            "\n",
            "More options:  ['Basotho', 'Beninese', 'Berber', 'Black African', 'Burundian', 'Cameroonian', 'Carthaginian', 'Chadian', 'Chewa', 'Congolese', 'Djiboutian', 'Egyptian', 'Ethiopian', 'Eurafrican', 'Ewe', 'Fulani'] \n",
            "\n",
            "\n",
            "2) Because the pharaoh was thought to be a god, government and religion were not separate in ancient  _______ .\n",
            "\t a )   Kuwait\n",
            "\t b )   Egypt\n",
            "\t c )   Saudi Arabia\n",
            "\t d )   Iraq\n",
            "\n",
            "More options:  ['Jordan', 'Israel', 'Fertile Crescent', 'Turkey', 'Iran', 'Lebanon', 'Shari', 'Mauritania', 'Nigeria', 'Somali peninsula', 'Sierra Leone', 'Malawi', 'North Africa', 'Senegal', 'Mozambique', 'Lake Tanganyika'] \n",
            "\n",
            "\n",
            "3) The  _______  provided so well for Egyptians that sometimes they had surpluses, or more goods than they needed.\n",
            "\t a )   Entebbe\n",
            "\t b )   Gulu\n",
            "\t c )   Buganda\n",
            "\t d )   Nile\n",
            "\n",
            "More options:  ['Jinja', 'Lake Edward', 'kayunga', 'gulu', 'entebbe', 'Port Sudan', 'Omdurman', 'Darfur', 'Libyan Desert', 'Kordofan', 'Khartoum', 'Nubian Desert', 'Nyala', 'Aswan High Dam', 'Eastern Desert', 'Aswan'] \n",
            "\n",
            "\n",
            "4) Nubia was the Egyptian name for the area of the  _______  Nile that had the richest gold mines in Africa.\n",
            "\t a )   Upper\n",
            "\t b )   Upper Berth\n",
            "\t c )   Lower Berth\n",
            "\n",
            "More options:  [] \n",
            "\n",
            "\n",
            "5) It combined the red  _______  of Lower Egypt with the white  _______  of Upper Egypt.\n",
            "\t a )   Crown\n",
            "\t b )   Capital\n",
            "\t c )   Head\n",
            "\t d )   Masthead\n",
            "\n",
            "More options:  [] \n",
            "\n",
            "\n",
            "6) Nubia was the Egyptian name for the area of the upper Nile that had the richest gold mines in  _______ .\n",
            "\t a )   Old World\n",
            "\t b )   Eurasia\n",
            "\t c )   Australia\n",
            "\t d )   Africa\n",
            "\n",
            "More options:  [] \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}